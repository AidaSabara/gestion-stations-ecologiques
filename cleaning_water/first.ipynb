{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e860a068-60b8-4261-8a4f-b7e24ebd8da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entree 1: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon ', 'Debit entree (m3/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µSm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azot total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "Entree FV1: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Debit entree (m3/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFV1a: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFV1b: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFV1c: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote toal (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "Entr√©√© FV2: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFV2a: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFV2b: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote toal (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFHa: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "SFHb: (16, 21), colonnes: ['Mois', 'Data', 'Heure echantillon /observations', 'Charge hydraulique (cm/jour)', 'Temperature (¬∫C)', 'pH', 'CE (¬µsm/cm)', 'Redox', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azote total (mgN/l)', 'Phosphates (mgPO4-/l)', 'Coliformes f√©caux (CFU/100ml)', 'Oeufs helmint', 'Huiles et graisses', 'Op√©ration', 'Observation']\n",
      "% √©l FV1: (28, 9), colonnes: ['FV1a', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azot total (mgN/l)', 'Phosphates (mgPO4-/l)']\n",
      "% √©l FV2: (19, 9), colonnes: ['SFV2a', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azot total (mgN/l)', 'Phosphates (mgPO4-/l)']\n",
      "% √©l de la station: (19, 9), colonnes: ['SFHa', 'DBO5 (mg/L)', 'DCO (mg/L)', 'MeS (mg/L)', 'MVS (%)', 'Nitrates (mgNO3-/l)', 'Ammonium (mgNH4-/l) ', 'Azot total (mgN/l)', 'Phosphates (mgPO4-/l)']\n"
     ]
    }
   ],
   "source": [
    "df_dict = pd.read_excel(\"Excel donn√©s qualit√© UGB.xlsx\", sheet_name=None, header=3)\n",
    "\n",
    "for name, data in df_dict.items():\n",
    "    print(f\"{name}: {data.shape}, colonnes: {list(data.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d6c5be-42c6-4913-b78a-1e7962287eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âBUT DU PR√âTRAITEMENT UGB ===\n",
      "Nombre de feuilles charg√©es : 13\n",
      "Traitement de la feuille : Entree 1\n",
      "  ‚úì Entree 1: 19 lignes\n",
      "Traitement de la feuille : Entree FV1\n",
      "  ‚úì Entree FV1: 19 lignes\n",
      "Traitement de la feuille : SFV1a\n",
      "  ‚úì SFV1a: 19 lignes\n",
      "Traitement de la feuille : SFV1b\n",
      "  ‚úì SFV1b: 19 lignes\n",
      "Traitement de la feuille : SFV1c\n",
      "  ‚úì SFV1c: 19 lignes\n",
      "Traitement de la feuille : Entr√©√© FV2\n",
      "  ‚úì Entr√©√© FV2: 19 lignes\n",
      "Traitement de la feuille : SFV2a\n",
      "  ‚úì SFV2a: 19 lignes\n",
      "Traitement de la feuille : SFV2b\n",
      "  ‚úì SFV2b: 19 lignes\n",
      "Traitement de la feuille : SFHa\n",
      "  ‚úì SFHa: 19 lignes\n",
      "Traitement de la feuille : SFHb\n",
      "  ‚úì SFHb: 19 lignes\n",
      "\n",
      "Dataset unifi√© cr√©√© : 190 lignes, 34 colonnes\n",
      "\n",
      "=== ANALYSE DES VALEURS MANQUANTES ===\n",
      "Top 10 des colonnes avec le plus de valeurs manquantes :\n",
      "       Colonne  Valeurs_Manquantes  Pourcentage_Manquant\n",
      "ENTREE station                 190                 100.0\n",
      "    Unnamed: 2                 190                 100.0\n",
      "    Entr√©e FV1                 190                 100.0\n",
      "    Unnamed: 3                 190                 100.0\n",
      "  Sortie  FV1c                 190                 100.0\n",
      "  Entr√©e  FV2a                 190                 100.0\n",
      "  Sortie  FV2a                 190                 100.0\n",
      "  Sortie  FV2b                 190                 100.0\n",
      "   Sortie  FHb                 190                 100.0\n",
      "   Sortie  Fha                 190                 100.0\n",
      "\n",
      "Colonnes num√©riques : 15\n",
      "Valeurs manquantes moyennes (param√®tres num√©riques) : 57.5%\n",
      "\n",
      "=== GESTION DES VALEURS MANQUANTES (strat√©gie: hybrid) ===\n",
      "Approche hybride intelligente...\n",
      "  ‚Üí Lignes vides supprim√©es : 68\n",
      "  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : 13\n",
      "    Colonnes : ['ENTREE station', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 19', 'Unnamed: 20', 'Entr√©e FV1', 'Sortie  FV1b', 'Sortie  FV1c', 'Entr√©e  FV2a', 'Sortie  FV2a', 'Sortie  FV2b', 'Sortie  Fha', 'Sortie  FHb']\n",
      "Shape avant : (190, 34)\n",
      "Shape apr√®s : (122, 22)\n",
      "Donn√©es conserv√©es : 64.2% des lignes, 64.7% des colonnes\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UGBDataProcessor' object has no attribute 'post_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 559\u001b[39m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TRAITEMENT TERMIN√â ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 519\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    516\u001b[39m processor = UGBDataProcessor()\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Traiter les donn√©es\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m result = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_unified_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m√âchec du traitement\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 247\u001b[39m, in \u001b[36mUGBDataProcessor.create_unified_dataset\u001b[39m\u001b[34m(self, excel_file_path)\u001b[39m\n\u001b[32m    235\u001b[39m df_unified = \u001b[38;5;28mself\u001b[39m.handle_missing_values(df_unified, strategy=\u001b[33m'\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Option 2 - CONSERVATIVE (d√©commenter si vous voulez garder toutes les donn√©es)\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# df_unified = self.handle_missing_values(df_unified, strategy='conservative')\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m \n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# 6. Post-traitement final\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m df_unified = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_process\u001b[49m(df_unified)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_unified, performance_sheets, missing_analysis\n",
      "\u001b[31mAttributeError\u001b[39m: 'UGBDataProcessor' object has no attribute 'post_process'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UGBDataProcessor:\n",
    "    \"\"\"\n",
    "    Classe pour le pr√©traitement des donn√©es de qualit√© d'eau UGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mapping pour uniformiser les noms de colonnes\n",
    "        self.column_mapping = {\n",
    "            # Colonnes temporelles\n",
    "            'Mois': 'Mois',\n",
    "            'Data': 'Date',\n",
    "            'Heure echantillon ': 'Heure_Echantillon',\n",
    "            'Heure echantillon /observations': 'Heure_Echantillon',\n",
    "            \n",
    "            # Param√®tres hydrauliques\n",
    "            'Debit entree (m3/jour)': 'Debit_Entree_m3_jour',\n",
    "            'Charge hydraulique (cm/jour)': 'Charge_Hydraulique_cm_jour',\n",
    "            \n",
    "            # Param√®tres physico-chimiques\n",
    "            'Temperature (¬∫C)': 'Temperature_C',\n",
    "            'pH': 'pH',\n",
    "            'CE (¬µSm/cm)': 'Conductivite_uS_cm',\n",
    "            'CE (¬µsm/cm)': 'Conductivite_uS_cm',\n",
    "            'Redox': 'Potentiel_Redox_mV',\n",
    "            \n",
    "            # Param√®tres de pollution\n",
    "            'DBO5 (mg/L)': 'DBO5_mg_L',\n",
    "            'DCO (mg/L)': 'DCO_mg_L',\n",
    "            'MeS (mg/L)': 'MES_mg_L',\n",
    "            'MVS (%)': 'MVS_pct',\n",
    "            \n",
    "            # Param√®tres azot√©s (TOUTES LES VARIANTES vers le m√™me nom)\n",
    "            'Nitrates (mgNO3-/l)': 'Nitrates_mg_L',\n",
    "            'Ammonium (mgNH4-/l) ': 'Ammonium_mg_L',\n",
    "            'Azot total (mgN/l)': 'Azote_Total_mg_L',      # Variante 1\n",
    "            'Azote total (mgN/l)': 'Azote_Total_mg_L',     # Variante 2 \n",
    "            'Azote toal (mgN/l)': 'Azote_Total_mg_L',      # Variante 3 (typo)\n",
    "            \n",
    "            # Param√®tres phosphor√©s\n",
    "            'Phosphates (mgPO4-/l)': 'Phosphates_mg_L',\n",
    "            \n",
    "            # Param√®tres microbiologiques\n",
    "            'Coliformes f√©caux (CFU/100ml)': 'Coliformes_Fecaux_CFU_100ml',\n",
    "            'Oeufs helmint': 'Oeufs_Helminthes',\n",
    "            \n",
    "            # Autres\n",
    "            'Huiles et graisses': 'Huiles_Graisses',\n",
    "            'Op√©ration': 'Operation',\n",
    "            'Observation': 'Observations'\n",
    "        }\n",
    "        \n",
    "        # D√©finition des types de feuilles et leurs caract√©ristiques\n",
    "        self.sheet_config = {\n",
    "            'entree': {\n",
    "                'sheets': ['Entree 1', 'Entree FV1', 'Entr√©√© FV2'],\n",
    "                'phase': 'Entree',\n",
    "                'description': 'Eaux d\\'entr√©e'\n",
    "            },\n",
    "            'sortie_fv': {\n",
    "                'sheets': ['SFV1a', 'SFV1b', 'SFV1c', 'SFV2a', 'SFV2b'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Vertical',\n",
    "                'description': 'Sorties filtres verticaux'\n",
    "            },\n",
    "            'sortie_fh': {\n",
    "                'sheets': ['SFHa', 'SFHb'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Horizontal',\n",
    "                'description': 'Sorties filtres horizontaux'\n",
    "            },\n",
    "            'performance': {\n",
    "                'sheets': ['% √©l FV1', '% √©l FV2', '% √©l de la station'],\n",
    "                'phase': 'Performance',\n",
    "                'description': 'Donn√©es de performance'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def clean_value(self, x):\n",
    "        \"\"\"\n",
    "        Nettoie une valeur individuelle\n",
    "        G√®re sp√©cifiquement : <2000, >500, \"pas de donn√©e\", etc.\n",
    "        \"\"\"\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        \n",
    "        if isinstance(x, str):\n",
    "            x = x.strip()\n",
    "            \n",
    "            # G√©rer les valeurs sp√©ciales (valeurs manquantes)\n",
    "            missing_indicators = ['pas de donn√©e', 'nd', 'n.d.', '-', 'absent', '', 'nan', 'null']\n",
    "            if any(term in x.lower() for term in missing_indicators):\n",
    "                return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec > (ex: \">500\" devient 500)\n",
    "            # Interpr√©tation: valeur minimale possible\n",
    "            if '>' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('>', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec < (ex: \"<2000\" devient 2000) \n",
    "            # Interpr√©tation: valeur maximale possible (limite de d√©tection)\n",
    "            if '<' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('<', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer d'autres formats possibles\n",
    "            # Retirer les espaces et caract√®res parasites\n",
    "            x = x.replace(',', '.').replace(' ', '')\n",
    "            \n",
    "            # Essayer de convertir en num√©rique\n",
    "            try:\n",
    "                return float(x)\n",
    "            except ValueError:\n",
    "                # Si ce n'est pas num√©rique, retourner tel quel (texte)\n",
    "                return x if x else np.nan\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def standardize_columns(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Standardise les noms de colonnes d'un DataFrame\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Renommer les colonnes selon le mapping\n",
    "        df_clean.rename(columns=self.column_mapping, inplace=True)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es\n",
    "        df_clean['ID_Station'] = 'Sanar_Station'\n",
    "        df_clean['Nom_Feuille'] = sheet_name\n",
    "        \n",
    "        # D√©terminer le type et la phase selon la feuille\n",
    "        for config_type, config in self.sheet_config.items():\n",
    "            if sheet_name in config['sheets']:\n",
    "                df_clean['Phase'] = config['phase']\n",
    "                if 'type_filtre' in config:\n",
    "                    df_clean['Type_Filtre'] = config['type_filtre']\n",
    "                else:\n",
    "                    df_clean['Type_Filtre'] = 'Non_Applicable'\n",
    "                break\n",
    "        \n",
    "        # Identifier le filtre sp√©cifique\n",
    "        if 'FV1' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV1'\n",
    "        elif 'FV2' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV2'\n",
    "        elif 'FH' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FH'\n",
    "        else:\n",
    "            df_clean['ID_Filtre'] = 'General'\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def process_sheet(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Traite une feuille individuelle\n",
    "        \"\"\"\n",
    "        print(f\"Traitement de la feuille : {sheet_name}\")\n",
    "        \n",
    "        # Standardiser les colonnes\n",
    "        df_processed = self.standardize_columns(df, sheet_name)\n",
    "        \n",
    "        # Nettoyer les valeurs (sauf les m√©tadonn√©es)\n",
    "        metadata_cols = ['ID_Station', 'Nom_Feuille', 'Phase', 'Type_Filtre', 'ID_Filtre']\n",
    "        \n",
    "        for col in df_processed.columns:\n",
    "            if col not in metadata_cols:\n",
    "                df_processed[col] = df_processed[col].apply(self.clean_value)\n",
    "        \n",
    "        # Traiter la date\n",
    "        if 'Date' in df_processed.columns:\n",
    "            df_processed['Date'] = pd.to_datetime(df_processed['Date'], errors='coerce')\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_unified_dataset(self, excel_file_path):\n",
    "        \"\"\"\n",
    "        Cr√©e un dataset unifi√© √† partir du fichier Excel\n",
    "        \"\"\"\n",
    "        print(\"=== D√âBUT DU PR√âTRAITEMENT UGB ===\")\n",
    "        \n",
    "        # 1. Charger toutes les feuilles\n",
    "        try:\n",
    "            df_dict = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "            print(f\"Nombre de feuilles charg√©es : {len(df_dict)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement : {e}\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Traiter chaque feuille (exclure les feuilles de performance pour l'instant)\n",
    "        processed_sheets = []\n",
    "        performance_sheets = []\n",
    "        \n",
    "        for sheet_name, data in df_dict.items():\n",
    "            if sheet_name.startswith('% √©l'):\n",
    "                # Traiter s√©par√©ment les feuilles de performance\n",
    "                performance_sheets.append((sheet_name, data))\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                processed_df = self.process_sheet(data, sheet_name)\n",
    "                processed_sheets.append(processed_df)\n",
    "                print(f\"  ‚úì {sheet_name}: {len(processed_df)} lignes\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Erreur avec {sheet_name}: {e}\")\n",
    "        \n",
    "        # 3. Combiner toutes les feuilles\n",
    "        if processed_sheets:\n",
    "            df_unified = pd.concat(processed_sheets, ignore_index=True, sort=False)\n",
    "            print(f\"\\nDataset unifi√© cr√©√© : {len(df_unified)} lignes, {len(df_unified.columns)} colonnes\")\n",
    "        else:\n",
    "            print(\"Aucune feuille trait√©e avec succ√®s\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Analyser les valeurs manquantes\n",
    "        missing_analysis = self.analyze_missing_values(df_unified)\n",
    "        \n",
    "        # 5. G√©rer les valeurs manquantes \n",
    "        # CHOISIR UNE SEULE STRAT√âGIE selon vos besoins :\n",
    "        \n",
    "        # Option 1 - HYBRID (Recommand√©e - approche intelligente)\n",
    "        df_unified = self.handle_missing_values(df_unified, strategy='hybrid')\n",
    "        \n",
    "        # Option 2 - CONSERVATIVE (d√©commenter si vous voulez garder toutes les donn√©es)\n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='conservative')\n",
    "        \n",
    "        # Option 3 - AGGRESSIVE (d√©commenter si vous voulez supprimer les colonnes tr√®s incompl√®tes)\n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='aggressive')\n",
    "        \n",
    "        # Option 4 - INTERPOLATE (d√©commenter si vous voulez interpoler les s√©ries temporelles)  \n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='interpolate')\n",
    "        \n",
    "        # 6. Post-traitement final\n",
    "        df_unified = self.post_process(df_unified)\n",
    "        \n",
    "        return df_unified, performance_sheets, missing_analysis\n",
    "    \n",
    "    def analyze_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Analyse d√©taill√©e des valeurs manquantes\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES VALEURS MANQUANTES ===\")\n",
    "        \n",
    "        # Compter les valeurs manquantes par colonne\n",
    "        missing_stats = []\n",
    "        for col in df.columns:\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            total_count = len(df)\n",
    "            missing_pct = (missing_count / total_count) * 100\n",
    "            \n",
    "            # Analyser les types de valeurs non-manquantes\n",
    "            non_missing = df[col].dropna()\n",
    "            if len(non_missing) > 0:\n",
    "                if col in ['Date', 'Mois', 'Heure_Echantillon', 'ID_Station', 'Phase', 'Type_Filtre']:\n",
    "                    data_type = 'M√©tadonn√©e'\n",
    "                elif non_missing.dtype in ['float64', 'int64']:\n",
    "                    data_type = 'Num√©rique'\n",
    "                else:\n",
    "                    data_type = 'Texte'\n",
    "            else:\n",
    "                data_type = 'Vide'\n",
    "            \n",
    "            missing_stats.append({\n",
    "                'Colonne': col,\n",
    "                'Valeurs_Manquantes': missing_count,\n",
    "                'Total': total_count,\n",
    "                'Pourcentage_Manquant': missing_pct,\n",
    "                'Type_Donnee': data_type\n",
    "            })\n",
    "        \n",
    "        missing_df = pd.DataFrame(missing_stats)\n",
    "        missing_df = missing_df.sort_values('Pourcentage_Manquant', ascending=False)\n",
    "        \n",
    "        # Afficher le top 10 des colonnes avec le plus de valeurs manquantes\n",
    "        print(\"Top 10 des colonnes avec le plus de valeurs manquantes :\")\n",
    "        print(missing_df.head(10)[['Colonne', 'Valeurs_Manquantes', 'Pourcentage_Manquant']].to_string(index=False))\n",
    "        \n",
    "        # Statistiques globales\n",
    "        colonnes_numeriques = missing_df[missing_df['Type_Donnee'] == 'Num√©rique']\n",
    "        if len(colonnes_numeriques) > 0:\n",
    "            print(f\"\\nColonnes num√©riques : {len(colonnes_numeriques)}\")\n",
    "            print(f\"Valeurs manquantes moyennes (param√®tres num√©riques) : {colonnes_numeriques['Pourcentage_Manquant'].mean():.1f}%\")\n",
    "        \n",
    "        return missing_df\n",
    "    \n",
    "    def handle_missing_values(self, df, strategy='hybrid'):\n",
    "        \"\"\"\n",
    "        G√®re les valeurs manquantes selon diff√©rentes strat√©gies\n",
    "        \n",
    "        Strat√©gies :\n",
    "        - 'conservative' : Garde les NaN, supprime les lignes compl√®tement vides\n",
    "        - 'aggressive' : Supprime les colonnes avec >80% de valeurs manquantes\n",
    "        - 'interpolate' : Interpole les valeurs manquantes pour les donn√©es temporelles\n",
    "        - 'hybrid' : Approche intelligente combin√©e (RECOMMAND√âE)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== GESTION DES VALEURS MANQUANTES (strat√©gie: {strategy}) ===\")\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        initial_shape = df_clean.shape\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # Supprimer les lignes compl√®tement vides (toutes les valeurs num√©riques sont NaN)\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            print(f\"Lignes compl√®tement vides supprim√©es : {initial_shape[0] - len(df_clean)}\")\n",
    "            \n",
    "        elif strategy == 'aggressive':\n",
    "            # Supprimer les colonnes avec plus de 80% de valeurs manquantes\n",
    "            threshold = 0.8\n",
    "            cols_to_drop = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date']:  # Garder les m√©tadonn√©es importantes\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > threshold:\n",
    "                        cols_to_drop.append(col)\n",
    "            \n",
    "            df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "            print(f\"Colonnes supprim√©es (>{threshold*100}% manquantes) : {len(cols_to_drop)}\")\n",
    "            if cols_to_drop:\n",
    "                print(f\"Colonnes supprim√©es : {cols_to_drop}\")\n",
    "            \n",
    "        elif strategy == 'interpolate':\n",
    "            # Interpolation pour les donn√©es temporelles\n",
    "            if 'Date' in df_clean.columns:\n",
    "                df_clean = df_clean.sort_values('Date')\n",
    "                numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                for col in numeric_cols:\n",
    "                    if col.endswith('_mg_L') or col.endswith('_C') or col.endswith('_pct'):\n",
    "                        # Interpolation lin√©aire pour les param√®tres de qualit√©\n",
    "                        df_clean[col] = df_clean.groupby(['Phase', 'ID_Filtre'])[col].transform(\n",
    "                            lambda x: x.interpolate(method='linear', limit=2)\n",
    "                        )\n",
    "        \n",
    "        elif strategy == 'hybrid':\n",
    "            # APPROCHE HYBRIDE - La meilleure pour les donn√©es de qualit√© d'eau\n",
    "            print(\"Approche hybride intelligente...\")\n",
    "            \n",
    "            # √âtape 1: Supprimer les lignes compl√®tement vides\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            rows_before = len(df_clean)\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            rows_removed = rows_before - len(df_clean)\n",
    "            print(f\"  ‚Üí Lignes vides supprim√©es : {rows_removed}\")\n",
    "            \n",
    "            # √âtape 2: Identifier les colonnes tr√®s probl√©matiques (>95% manquantes)\n",
    "            very_empty_cols = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date', 'Mois']:\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > 0.95:  # Plus strict que aggressive\n",
    "                        very_empty_cols.append(col)\n",
    "            \n",
    "            if very_empty_cols:\n",
    "                df_clean = df_clean.drop(columns=very_empty_cols)\n",
    "                print(f\"  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : {len(very_empty_cols)}\")\n",
    "                print(f\"    Colonnes : {very_empty_cols}\")\n",
    "            \n",
    "            # √âtape 3: Interpolation douce pour les param√®tres physiques stables (pH, Temp√©rature)\n",
    "            stable_params = ['Temperature_C', 'pH', 'Conductivite_uS_cm']\n",
    "            interpolated_count = 0\n",
    "            \n",
    "            for param in stable_params:\n",
    "                if param in df_clean.columns and 'Date' in df_clean.columns:\n",
    "                    # Interpolation seulement si max 1 valeur manquante cons√©cutive\n",
    "                    df_clean = df_clean.sort_values(['Date', 'Phase', 'ID_Filtre'])\n",
    "                    original_nulls = df_clean[param].isnull().sum()\n",
    "                    \n",
    "                    df_clean[param] = df_clean.groupby(['Phase', 'ID_Filtre'])[param].transform(\n",
    "                        lambda x: x.interpolate(method='linear', limit=1)  # Max 1 trou\n",
    "                    )\n",
    "                    \n",
    "                    final_nulls = df_clean[param].isnull().sum()\n",
    "                    if original_nulls > final_nulls:\n",
    "                        interpolated_count += (original_nulls - final_nulls)\n",
    "            \n",
    "            if interpolated_count > 0:\n",
    "                print(f\"  ‚Üí Valeurs interpol√©es (param√®tres stables) : {interpolated_count}\")\n",
    "            \n",
    "            # √âtape 4: Marquer les valeurs estim√©es vs mesur√©es\n",
    "            df_clean['Contient_Valeurs_Estimees'] = False\n",
    "            for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "                if col not in ['ID_Station']:\n",
    "                    # Marquer les lignes o√π au moins 30% des param√®tres principaux sont manquants\n",
    "                    main_params = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'pH', 'Temperature_C']\n",
    "                    available_main = [p for p in main_params if p in df_clean.columns]\n",
    "                    \n",
    "                    if available_main:\n",
    "                        missing_main = df_clean[available_main].isnull().sum(axis=1)\n",
    "                        df_clean.loc[missing_main >= len(available_main) * 0.3, 'Contient_Valeurs_Estimees'] = True\n",
    "        \n",
    "        final_shape = df_clean.shape\n",
    "        print(f\"Shape avant : {initial_shape}\")\n",
    "        print(f\"Shape apr√®s : {final_shape}\")\n",
    "        print(f\"Donn√©es conserv√©es : {(final_shape[0]/initial_shape[0]*100):.1f}% des lignes, \"\n",
    "              f\"{(final_shape[1]/initial_shape[1]*100):.1f}% des colonnes\")\n",
    "        \n",
    "        return df_clean\n",
    "        \"\"\"\n",
    "        Post-traitement du dataset unifi√©\n",
    "        \"\"\"\n",
    "        print(\"\\n=== POST-TRAITEMENT ===\")\n",
    "        \n",
    "        # R√©organiser les colonnes\n",
    "        priority_cols = ['ID_Station', 'Phase', 'Type_Filtre', 'ID_Filtre', 'Date', 'Mois', 'Heure_Echantillon']\n",
    "        other_cols = [col for col in df.columns if col not in priority_cols]\n",
    "        df = df[priority_cols + other_cols]\n",
    "        \n",
    "        # Trier par date\n",
    "        if 'Date' in df.columns:\n",
    "            df = df.sort_values(['Date', 'Phase', 'ID_Filtre']).reset_index(drop=True)\n",
    "        \n",
    "        # Statistiques de nettoyage\n",
    "        total_cells = df.shape[0] * df.shape[1]\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"Donn√©es manquantes : {missing_cells}/{total_cells} ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        # R√©sum√© par phase\n",
    "        phase_summary = df.groupby('Phase').size()\n",
    "        print(\"R√©partition par phase :\")\n",
    "        for phase, count in phase_summary.items():\n",
    "            print(f\"  - {phase}: {count} lignes\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_performance_analysis(self, df_unified, performance_sheets):\n",
    "        \"\"\"\n",
    "        Analyse les donn√©es de performance si disponibles\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES PERFORMANCES ===\")\n",
    "        \n",
    "        # Cr√©er des paires entr√©e-sortie pour calcul des rendements\n",
    "        paires_filtres = {\n",
    "            'FV1': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV1', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV1')]\n",
    "            },\n",
    "            'FV2': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV2', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV2')]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        performance_results = []\n",
    "        \n",
    "        for filtre_id, data in paires_filtres.items():\n",
    "            if len(data['entree']) > 0 and len(data['sortie']) > 0:\n",
    "                print(f\"Calcul des rendements pour {filtre_id}\")\n",
    "                \n",
    "                # Fusionner entr√©e et sortie sur la date\n",
    "                merged = pd.merge(\n",
    "                    data['entree'], \n",
    "                    data['sortie'], \n",
    "                    on='Date', \n",
    "                    suffixes=('_entree', '_sortie'),\n",
    "                    how='inner'\n",
    "                )\n",
    "                \n",
    "                if len(merged) > 0:\n",
    "                    # Calculer les rendements pour les param√®tres cl√©s\n",
    "                    parametres = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'Azote_Total_mg_L', 'Phosphates_mg_L']\n",
    "                    \n",
    "                    for param in parametres:\n",
    "                        col_entree = f\"{param}_entree\"\n",
    "                        col_sortie = f\"{param}_sortie\"\n",
    "                        \n",
    "                        if col_entree in merged.columns and col_sortie in merged.columns:\n",
    "                            merged[f\"Rendement_{param.replace('_mg_L', '')}_pct\"] = (\n",
    "                                (merged[col_entree] - merged[col_sortie]) / \n",
    "                                merged[col_entree] * 100\n",
    "                            ).round(2)\n",
    "                    \n",
    "                    performance_results.append(merged)\n",
    "        \n",
    "        return performance_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale\n",
    "    \"\"\"\n",
    "    # V√©rifier que le fichier existe\n",
    "    excel_file = \"Excel donn√©s qualit√© UGB.xlsx\"\n",
    "    \n",
    "    if not Path(excel_file).exists():\n",
    "        print(f\"‚ùå ERREUR : Fichier '{excel_file}' introuvable !\")\n",
    "        print(\"üìÅ Fichiers Excel trouv√©s dans le r√©pertoire actuel :\")\n",
    "        excel_files = list(Path('.').glob('*.xlsx'))\n",
    "        if excel_files:\n",
    "            for i, file in enumerate(excel_files, 1):\n",
    "                print(f\"   {i}. {file.name}\")\n",
    "            print(\"\\nüí° Conseil : V√©rifiez le nom exact du fichier ci-dessus\")\n",
    "        else:\n",
    "            print(\"   Aucun fichier .xlsx trouv√©\")\n",
    "        return\n",
    "    \n",
    "    # Initialiser le processeur\n",
    "    processor = UGBDataProcessor()\n",
    "    \n",
    "    # Traiter les donn√©es\n",
    "    result = processor.create_unified_dataset(excel_file)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"√âchec du traitement\")\n",
    "        return\n",
    "    \n",
    "    df_unified, performance_sheets, missing_analysis = result\n",
    "    \n",
    "    # Afficher un aper√ßu\n",
    "    print(\"\\n=== APER√áU DU DATASET FINAL ===\")\n",
    "    print(f\"Shape: {df_unified.shape}\")\n",
    "    print(\"\\nPremi√®res lignes :\")\n",
    "    print(df_unified.head())\n",
    "    \n",
    "    print(\"\\nColonnes disponibles :\")\n",
    "    for i, col in enumerate(df_unified.columns):\n",
    "        print(f\"{i+1:2d}. {col}\")\n",
    "    \n",
    "    # Sauvegarder le dataset principal en CSV\n",
    "    output_file_csv = \"UGB_Sanar_Station_Dataset_Clean.csv\"\n",
    "    df_unified.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Dataset principal sauvegard√© : {output_file_csv}\")\n",
    "    \n",
    "    # Sauvegarder l'analyse des valeurs manquantes en CSV\n",
    "    missing_file_csv = \"UGB_Missing_Values_Analysis.csv\"\n",
    "    missing_analysis.to_csv(missing_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Analyse des valeurs manquantes : {missing_file_csv}\")\n",
    "    \n",
    "    # Analyser les performances si demand√©\n",
    "    performance_results = processor.create_performance_analysis(df_unified, performance_sheets)\n",
    "    \n",
    "    if performance_results:\n",
    "        df_performance = pd.concat(performance_results, ignore_index=True)\n",
    "        performance_file_csv = \"UGB_Performance_Analysis.csv\"\n",
    "        df_performance.to_csv(performance_file_csv, index=False, encoding='utf-8')\n",
    "        print(f\"‚úì Analyse des performances : {performance_file_csv}\")\n",
    "    \n",
    "    print(\"\\n=== TRAITEMENT TERMIN√â ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1382eb03-3969-46ee-a7fd-17737988fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âBUT DU PR√âTRAITEMENT UGB ===\n",
      "Nombre de feuilles charg√©es : 13\n",
      "Traitement de la feuille : Entree 1\n",
      "  ‚úì Entree 1: 19 lignes\n",
      "Traitement de la feuille : Entree FV1\n",
      "  ‚úì Entree FV1: 19 lignes\n",
      "Traitement de la feuille : SFV1a\n",
      "  ‚úì SFV1a: 19 lignes\n",
      "Traitement de la feuille : SFV1b\n",
      "  ‚úì SFV1b: 19 lignes\n",
      "Traitement de la feuille : SFV1c\n",
      "  ‚úì SFV1c: 19 lignes\n",
      "Traitement de la feuille : Entr√©√© FV2\n",
      "  ‚úì Entr√©√© FV2: 19 lignes\n",
      "Traitement de la feuille : SFV2a\n",
      "  ‚úì SFV2a: 19 lignes\n",
      "Traitement de la feuille : SFV2b\n",
      "  ‚úì SFV2b: 19 lignes\n",
      "Traitement de la feuille : SFHa\n",
      "  ‚úì SFHa: 19 lignes\n",
      "Traitement de la feuille : SFHb\n",
      "  ‚úì SFHb: 19 lignes\n",
      "\n",
      "Dataset unifi√© cr√©√© : 190 lignes, 34 colonnes\n",
      "\n",
      "=== ANALYSE DES VALEURS MANQUANTES ===\n",
      "Top 10 des colonnes avec le plus de valeurs manquantes :\n",
      "       Colonne  Valeurs_Manquantes  Pourcentage_Manquant\n",
      "ENTREE station                 190                 100.0\n",
      "    Unnamed: 2                 190                 100.0\n",
      "    Entr√©e FV1                 190                 100.0\n",
      "    Unnamed: 3                 190                 100.0\n",
      "  Sortie  FV1c                 190                 100.0\n",
      "  Entr√©e  FV2a                 190                 100.0\n",
      "  Sortie  FV2a                 190                 100.0\n",
      "  Sortie  FV2b                 190                 100.0\n",
      "   Sortie  FHb                 190                 100.0\n",
      "   Sortie  Fha                 190                 100.0\n",
      "\n",
      "Colonnes num√©riques : 15\n",
      "Valeurs manquantes moyennes (param√®tres num√©riques) : 57.5%\n",
      "\n",
      "=== GESTION DES VALEURS MANQUANTES (strat√©gie: hybrid) ===\n",
      "Approche hybride intelligente...\n",
      "  ‚Üí Lignes vides supprim√©es : 68\n",
      "  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : 13\n",
      "    Colonnes : ['ENTREE station', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 19', 'Unnamed: 20', 'Entr√©e FV1', 'Sortie  FV1b', 'Sortie  FV1c', 'Entr√©e  FV2a', 'Sortie  FV2a', 'Sortie  FV2b', 'Sortie  Fha', 'Sortie  FHb']\n",
      "Shape avant : (190, 34)\n",
      "Shape apr√®s : (122, 22)\n",
      "Donn√©es conserv√©es : 64.2% des lignes, 64.7% des colonnes\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UGBDataProcessor' object has no attribute 'post_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 559\u001b[39m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TRAITEMENT TERMIN√â ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 519\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    516\u001b[39m processor = UGBDataProcessor()\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Traiter les donn√©es\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m result = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_unified_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m√âchec du traitement\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 247\u001b[39m, in \u001b[36mUGBDataProcessor.create_unified_dataset\u001b[39m\u001b[34m(self, excel_file_path)\u001b[39m\n\u001b[32m    235\u001b[39m df_unified = \u001b[38;5;28mself\u001b[39m.handle_missing_values(df_unified, strategy=\u001b[33m'\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Option 2 - CONSERVATIVE (d√©commenter si vous voulez garder toutes les donn√©es)\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# df_unified = self.handle_missing_values(df_unified, strategy='conservative')\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m \n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# 6. Post-traitement final\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m df_unified = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_process\u001b[49m(df_unified)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_unified, performance_sheets, missing_analysis\n",
      "\u001b[31mAttributeError\u001b[39m: 'UGBDataProcessor' object has no attribute 'post_process'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UGBDataProcessor:\n",
    "    \"\"\"\n",
    "    Classe pour le pr√©traitement des donn√©es de qualit√© d'eau UGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mapping pour uniformiser les noms de colonnes\n",
    "        self.column_mapping = {\n",
    "            # Colonnes temporelles\n",
    "            'Mois': 'Mois',\n",
    "            'Data': 'Date',\n",
    "            'Heure echantillon ': 'Heure_Echantillon',\n",
    "            'Heure echantillon /observations': 'Heure_Echantillon',\n",
    "            \n",
    "            # Param√®tres hydrauliques\n",
    "            'Debit entree (m3/jour)': 'Debit_Entree_m3_jour',\n",
    "            'Charge hydraulique (cm/jour)': 'Charge_Hydraulique_cm_jour',\n",
    "            \n",
    "            # Param√®tres physico-chimiques\n",
    "            'Temperature (¬∫C)': 'Temperature_C',\n",
    "            'pH': 'pH',\n",
    "            'CE (¬µSm/cm)': 'Conductivite_uS_cm',\n",
    "            'CE (¬µsm/cm)': 'Conductivite_uS_cm',\n",
    "            'Redox': 'Potentiel_Redox_mV',\n",
    "            \n",
    "            # Param√®tres de pollution\n",
    "            'DBO5 (mg/L)': 'DBO5_mg_L',\n",
    "            'DCO (mg/L)': 'DCO_mg_L',\n",
    "            'MeS (mg/L)': 'MES_mg_L',\n",
    "            'MVS (%)': 'MVS_pct',\n",
    "            \n",
    "            # Param√®tres azot√©s (TOUTES LES VARIANTES vers le m√™me nom)\n",
    "            'Nitrates (mgNO3-/l)': 'Nitrates_mg_L',\n",
    "            'Ammonium (mgNH4-/l) ': 'Ammonium_mg_L',\n",
    "            'Azot total (mgN/l)': 'Azote_Total_mg_L',      # Variante 1\n",
    "            'Azote total (mgN/l)': 'Azote_Total_mg_L',     # Variante 2 \n",
    "            'Azote toal (mgN/l)': 'Azote_Total_mg_L',      # Variante 3 (typo)\n",
    "            \n",
    "            # Param√®tres phosphor√©s\n",
    "            'Phosphates (mgPO4-/l)': 'Phosphates_mg_L',\n",
    "            \n",
    "            # Param√®tres microbiologiques\n",
    "            'Coliformes f√©caux (CFU/100ml)': 'Coliformes_Fecaux_CFU_100ml',\n",
    "            'Oeufs helmint': 'Oeufs_Helminthes',\n",
    "            \n",
    "            # Autres\n",
    "            'Huiles et graisses': 'Huiles_Graisses',\n",
    "            'Op√©ration': 'Operation',\n",
    "            'Observation': 'Observations'\n",
    "        }\n",
    "        \n",
    "        # D√©finition des types de feuilles et leurs caract√©ristiques\n",
    "        self.sheet_config = {\n",
    "            'entree': {\n",
    "                'sheets': ['Entree 1', 'Entree FV1', 'Entr√©√© FV2'],\n",
    "                'phase': 'Entree',\n",
    "                'description': 'Eaux d\\'entr√©e'\n",
    "            },\n",
    "            'sortie_fv': {\n",
    "                'sheets': ['SFV1a', 'SFV1b', 'SFV1c', 'SFV2a', 'SFV2b'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Vertical',\n",
    "                'description': 'Sorties filtres verticaux'\n",
    "            },\n",
    "            'sortie_fh': {\n",
    "                'sheets': ['SFHa', 'SFHb'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Horizontal',\n",
    "                'description': 'Sorties filtres horizontaux'\n",
    "            },\n",
    "            'performance': {\n",
    "                'sheets': ['% √©l FV1', '% √©l FV2', '% √©l de la station'],\n",
    "                'phase': 'Performance',\n",
    "                'description': 'Donn√©es de performance'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def clean_value(self, x):\n",
    "        \"\"\"\n",
    "        Nettoie une valeur individuelle\n",
    "        G√®re sp√©cifiquement : <2000, >500, \"pas de donn√©e\", etc.\n",
    "        \"\"\"\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        \n",
    "        if isinstance(x, str):\n",
    "            x = x.strip()\n",
    "            \n",
    "            # G√©rer les valeurs sp√©ciales (valeurs manquantes)\n",
    "            missing_indicators = ['pas de donn√©e', 'nd', 'n.d.', '-', 'absent', '', 'nan', 'null']\n",
    "            if any(term in x.lower() for term in missing_indicators):\n",
    "                return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec > (ex: \">500\" devient 500)\n",
    "            # Interpr√©tation: valeur minimale possible\n",
    "            if '>' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('>', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec < (ex: \"<2000\" devient 2000) \n",
    "            # Interpr√©tation: valeur maximale possible (limite de d√©tection)\n",
    "            if '<' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('<', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer d'autres formats possibles\n",
    "            # Retirer les espaces et caract√®res parasites\n",
    "            x = x.replace(',', '.').replace(' ', '')\n",
    "            \n",
    "            # Essayer de convertir en num√©rique\n",
    "            try:\n",
    "                return float(x)\n",
    "            except ValueError:\n",
    "                # Si ce n'est pas num√©rique, retourner tel quel (texte)\n",
    "                return x if x else np.nan\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def standardize_columns(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Standardise les noms de colonnes d'un DataFrame\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Renommer les colonnes selon le mapping\n",
    "        df_clean.rename(columns=self.column_mapping, inplace=True)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es\n",
    "        df_clean['ID_Station'] = 'Sanar_Station'\n",
    "        df_clean['Nom_Feuille'] = sheet_name\n",
    "        \n",
    "        # D√©terminer le type et la phase selon la feuille\n",
    "        for config_type, config in self.sheet_config.items():\n",
    "            if sheet_name in config['sheets']:\n",
    "                df_clean['Phase'] = config['phase']\n",
    "                if 'type_filtre' in config:\n",
    "                    df_clean['Type_Filtre'] = config['type_filtre']\n",
    "                else:\n",
    "                    df_clean['Type_Filtre'] = 'Non_Applicable'\n",
    "                break\n",
    "        \n",
    "        # Identifier le filtre sp√©cifique\n",
    "        if 'FV1' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV1'\n",
    "        elif 'FV2' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV2'\n",
    "        elif 'FH' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FH'\n",
    "        else:\n",
    "            df_clean['ID_Filtre'] = 'General'\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def process_sheet(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Traite une feuille individuelle\n",
    "        \"\"\"\n",
    "        print(f\"Traitement de la feuille : {sheet_name}\")\n",
    "        \n",
    "        # Standardiser les colonnes\n",
    "        df_processed = self.standardize_columns(df, sheet_name)\n",
    "        \n",
    "        # Nettoyer les valeurs (sauf les m√©tadonn√©es)\n",
    "        metadata_cols = ['ID_Station', 'Nom_Feuille', 'Phase', 'Type_Filtre', 'ID_Filtre']\n",
    "        \n",
    "        for col in df_processed.columns:\n",
    "            if col not in metadata_cols:\n",
    "                df_processed[col] = df_processed[col].apply(self.clean_value)\n",
    "        \n",
    "        # Traiter la date\n",
    "        if 'Date' in df_processed.columns:\n",
    "            df_processed['Date'] = pd.to_datetime(df_processed['Date'], errors='coerce')\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_unified_dataset(self, excel_file_path):\n",
    "        \"\"\"\n",
    "        Cr√©e un dataset unifi√© √† partir du fichier Excel\n",
    "        \"\"\"\n",
    "        print(\"=== D√âBUT DU PR√âTRAITEMENT UGB ===\")\n",
    "        \n",
    "        # 1. Charger toutes les feuilles\n",
    "        try:\n",
    "            df_dict = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "            print(f\"Nombre de feuilles charg√©es : {len(df_dict)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement : {e}\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Traiter chaque feuille (exclure les feuilles de performance pour l'instant)\n",
    "        processed_sheets = []\n",
    "        performance_sheets = []\n",
    "        \n",
    "        for sheet_name, data in df_dict.items():\n",
    "            if sheet_name.startswith('% √©l'):\n",
    "                # Traiter s√©par√©ment les feuilles de performance\n",
    "                performance_sheets.append((sheet_name, data))\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                processed_df = self.process_sheet(data, sheet_name)\n",
    "                processed_sheets.append(processed_df)\n",
    "                print(f\"  ‚úì {sheet_name}: {len(processed_df)} lignes\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Erreur avec {sheet_name}: {e}\")\n",
    "        \n",
    "        # 3. Combiner toutes les feuilles\n",
    "        if processed_sheets:\n",
    "            df_unified = pd.concat(processed_sheets, ignore_index=True, sort=False)\n",
    "            print(f\"\\nDataset unifi√© cr√©√© : {len(df_unified)} lignes, {len(df_unified.columns)} colonnes\")\n",
    "        else:\n",
    "            print(\"Aucune feuille trait√©e avec succ√®s\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Analyser les valeurs manquantes\n",
    "        missing_analysis = self.analyze_missing_values(df_unified)\n",
    "        \n",
    "        # 5. G√©rer les valeurs manquantes \n",
    "        # CHOISIR UNE SEULE STRAT√âGIE selon vos besoins :\n",
    "        \n",
    "        # Option 1 - HYBRID (Recommand√©e - approche intelligente)\n",
    "        df_unified = self.handle_missing_values(df_unified, strategy='hybrid')\n",
    "        \n",
    "        # Option 2 - CONSERVATIVE (d√©commenter si vous voulez garder toutes les donn√©es)\n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='conservative')\n",
    "        \n",
    "        # Option 3 - AGGRESSIVE (d√©commenter si vous voulez supprimer les colonnes tr√®s incompl√®tes)\n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='aggressive')\n",
    "        \n",
    "        # Option 4 - INTERPOLATE (d√©commenter si vous voulez interpoler les s√©ries temporelles)  \n",
    "        # df_unified = self.handle_missing_values(df_unified, strategy='interpolate')\n",
    "        \n",
    "        # 6. Post-traitement final\n",
    "        df_unified = self.post_process(df_unified)\n",
    "        \n",
    "        return df_unified, performance_sheets, missing_analysis\n",
    "    \n",
    "    def analyze_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Analyse d√©taill√©e des valeurs manquantes\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES VALEURS MANQUANTES ===\")\n",
    "        \n",
    "        # Compter les valeurs manquantes par colonne\n",
    "        missing_stats = []\n",
    "        for col in df.columns:\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            total_count = len(df)\n",
    "            missing_pct = (missing_count / total_count) * 100\n",
    "            \n",
    "            # Analyser les types de valeurs non-manquantes\n",
    "            non_missing = df[col].dropna()\n",
    "            if len(non_missing) > 0:\n",
    "                if col in ['Date', 'Mois', 'Heure_Echantillon', 'ID_Station', 'Phase', 'Type_Filtre']:\n",
    "                    data_type = 'M√©tadonn√©e'\n",
    "                elif non_missing.dtype in ['float64', 'int64']:\n",
    "                    data_type = 'Num√©rique'\n",
    "                else:\n",
    "                    data_type = 'Texte'\n",
    "            else:\n",
    "                data_type = 'Vide'\n",
    "            \n",
    "            missing_stats.append({\n",
    "                'Colonne': col,\n",
    "                'Valeurs_Manquantes': missing_count,\n",
    "                'Total': total_count,\n",
    "                'Pourcentage_Manquant': missing_pct,\n",
    "                'Type_Donnee': data_type\n",
    "            })\n",
    "        \n",
    "        missing_df = pd.DataFrame(missing_stats)\n",
    "        missing_df = missing_df.sort_values('Pourcentage_Manquant', ascending=False)\n",
    "        \n",
    "        # Afficher le top 10 des colonnes avec le plus de valeurs manquantes\n",
    "        print(\"Top 10 des colonnes avec le plus de valeurs manquantes :\")\n",
    "        print(missing_df.head(10)[['Colonne', 'Valeurs_Manquantes', 'Pourcentage_Manquant']].to_string(index=False))\n",
    "        \n",
    "        # Statistiques globales\n",
    "        colonnes_numeriques = missing_df[missing_df['Type_Donnee'] == 'Num√©rique']\n",
    "        if len(colonnes_numeriques) > 0:\n",
    "            print(f\"\\nColonnes num√©riques : {len(colonnes_numeriques)}\")\n",
    "            print(f\"Valeurs manquantes moyennes (param√®tres num√©riques) : {colonnes_numeriques['Pourcentage_Manquant'].mean():.1f}%\")\n",
    "        \n",
    "        return missing_df\n",
    "    \n",
    "    def handle_missing_values(self, df, strategy='hybrid'):\n",
    "        \"\"\"\n",
    "        G√®re les valeurs manquantes selon diff√©rentes strat√©gies\n",
    "        \n",
    "        Strat√©gies :\n",
    "        - 'conservative' : Garde les NaN, supprime les lignes compl√®tement vides\n",
    "        - 'aggressive' : Supprime les colonnes avec >80% de valeurs manquantes\n",
    "        - 'interpolate' : Interpole les valeurs manquantes pour les donn√©es temporelles\n",
    "        - 'hybrid' : Approche intelligente combin√©e (RECOMMAND√âE)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== GESTION DES VALEURS MANQUANTES (strat√©gie: {strategy}) ===\")\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        initial_shape = df_clean.shape\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # Supprimer les lignes compl√®tement vides (toutes les valeurs num√©riques sont NaN)\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            print(f\"Lignes compl√®tement vides supprim√©es : {initial_shape[0] - len(df_clean)}\")\n",
    "            \n",
    "        elif strategy == 'aggressive':\n",
    "            # Supprimer les colonnes avec plus de 80% de valeurs manquantes\n",
    "            threshold = 0.8\n",
    "            cols_to_drop = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date']:  # Garder les m√©tadonn√©es importantes\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > threshold:\n",
    "                        cols_to_drop.append(col)\n",
    "            \n",
    "            df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "            print(f\"Colonnes supprim√©es (>{threshold*100}% manquantes) : {len(cols_to_drop)}\")\n",
    "            if cols_to_drop:\n",
    "                print(f\"Colonnes supprim√©es : {cols_to_drop}\")\n",
    "            \n",
    "        elif strategy == 'interpolate':\n",
    "            # Interpolation pour les donn√©es temporelles\n",
    "            if 'Date' in df_clean.columns:\n",
    "                df_clean = df_clean.sort_values('Date')\n",
    "                numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "                \n",
    "                for col in numeric_cols:\n",
    "                    if col.endswith('_mg_L') or col.endswith('_C') or col.endswith('_pct'):\n",
    "                        # Interpolation lin√©aire pour les param√®tres de qualit√©\n",
    "                        df_clean[col] = df_clean.groupby(['Phase', 'ID_Filtre'])[col].transform(\n",
    "                            lambda x: x.interpolate(method='linear', limit=2)\n",
    "                        )\n",
    "        \n",
    "        elif strategy == 'hybrid':\n",
    "            # APPROCHE HYBRIDE - La meilleure pour les donn√©es de qualit√© d'eau\n",
    "            print(\"Approche hybride intelligente...\")\n",
    "            \n",
    "            # √âtape 1: Supprimer les lignes compl√®tement vides\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            rows_before = len(df_clean)\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            rows_removed = rows_before - len(df_clean)\n",
    "            print(f\"  ‚Üí Lignes vides supprim√©es : {rows_removed}\")\n",
    "            \n",
    "            # √âtape 2: Identifier les colonnes tr√®s probl√©matiques (>95% manquantes)\n",
    "            very_empty_cols = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date', 'Mois']:\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > 0.95:  # Plus strict que aggressive\n",
    "                        very_empty_cols.append(col)\n",
    "            \n",
    "            if very_empty_cols:\n",
    "                df_clean = df_clean.drop(columns=very_empty_cols)\n",
    "                print(f\"  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : {len(very_empty_cols)}\")\n",
    "                print(f\"    Colonnes : {very_empty_cols}\")\n",
    "            \n",
    "            # √âtape 3: Interpolation douce pour les param√®tres physiques stables (pH, Temp√©rature)\n",
    "            stable_params = ['Temperature_C', 'pH', 'Conductivite_uS_cm']\n",
    "            interpolated_count = 0\n",
    "            \n",
    "            for param in stable_params:\n",
    "                if param in df_clean.columns and 'Date' in df_clean.columns:\n",
    "                    # Interpolation seulement si max 1 valeur manquante cons√©cutive\n",
    "                    df_clean = df_clean.sort_values(['Date', 'Phase', 'ID_Filtre'])\n",
    "                    original_nulls = df_clean[param].isnull().sum()\n",
    "                    \n",
    "                    df_clean[param] = df_clean.groupby(['Phase', 'ID_Filtre'])[param].transform(\n",
    "                        lambda x: x.interpolate(method='linear', limit=1)  # Max 1 trou\n",
    "                    )\n",
    "                    \n",
    "                    final_nulls = df_clean[param].isnull().sum()\n",
    "                    if original_nulls > final_nulls:\n",
    "                        interpolated_count += (original_nulls - final_nulls)\n",
    "            \n",
    "            if interpolated_count > 0:\n",
    "                print(f\"  ‚Üí Valeurs interpol√©es (param√®tres stables) : {interpolated_count}\")\n",
    "            \n",
    "            # √âtape 4: Marquer les valeurs estim√©es vs mesur√©es\n",
    "            df_clean['Contient_Valeurs_Estimees'] = False\n",
    "            for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "                if col not in ['ID_Station']:\n",
    "                    # Marquer les lignes o√π au moins 30% des param√®tres principaux sont manquants\n",
    "                    main_params = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'pH', 'Temperature_C']\n",
    "                    available_main = [p for p in main_params if p in df_clean.columns]\n",
    "                    \n",
    "                    if available_main:\n",
    "                        missing_main = df_clean[available_main].isnull().sum(axis=1)\n",
    "                        df_clean.loc[missing_main >= len(available_main) * 0.3, 'Contient_Valeurs_Estimees'] = True\n",
    "        \n",
    "        final_shape = df_clean.shape\n",
    "        print(f\"Shape avant : {initial_shape}\")\n",
    "        print(f\"Shape apr√®s : {final_shape}\")\n",
    "        print(f\"Donn√©es conserv√©es : {(final_shape[0]/initial_shape[0]*100):.1f}% des lignes, \"\n",
    "              f\"{(final_shape[1]/initial_shape[1]*100):.1f}% des colonnes\")\n",
    "        \n",
    "        return df_clean\n",
    "        \"\"\"\n",
    "        Post-traitement du dataset unifi√©\n",
    "        \"\"\"\n",
    "        print(\"\\n=== POST-TRAITEMENT ===\")\n",
    "        \n",
    "        # R√©organiser les colonnes\n",
    "        priority_cols = ['ID_Station', 'Phase', 'Type_Filtre', 'ID_Filtre', 'Date', 'Mois', 'Heure_Echantillon']\n",
    "        other_cols = [col for col in df.columns if col not in priority_cols]\n",
    "        df = df[priority_cols + other_cols]\n",
    "        \n",
    "        # Trier par date\n",
    "        if 'Date' in df.columns:\n",
    "            df = df.sort_values(['Date', 'Phase', 'ID_Filtre']).reset_index(drop=True)\n",
    "        \n",
    "        # Statistiques de nettoyage\n",
    "        total_cells = df.shape[0] * df.shape[1]\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"Donn√©es manquantes : {missing_cells}/{total_cells} ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        # R√©sum√© par phase\n",
    "        phase_summary = df.groupby('Phase').size()\n",
    "        print(\"R√©partition par phase :\")\n",
    "        for phase, count in phase_summary.items():\n",
    "            print(f\"  - {phase}: {count} lignes\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_performance_analysis(self, df_unified, performance_sheets):\n",
    "        \"\"\"\n",
    "        Analyse les donn√©es de performance si disponibles\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES PERFORMANCES ===\")\n",
    "        \n",
    "        # Cr√©er des paires entr√©e-sortie pour calcul des rendements\n",
    "        paires_filtres = {\n",
    "            'FV1': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV1', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV1')]\n",
    "            },\n",
    "            'FV2': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV2', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV2')]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        performance_results = []\n",
    "        \n",
    "        for filtre_id, data in paires_filtres.items():\n",
    "            if len(data['entree']) > 0 and len(data['sortie']) > 0:\n",
    "                print(f\"Calcul des rendements pour {filtre_id}\")\n",
    "                \n",
    "                # Fusionner entr√©e et sortie sur la date\n",
    "                merged = pd.merge(\n",
    "                    data['entree'], \n",
    "                    data['sortie'], \n",
    "                    on='Date', \n",
    "                    suffixes=('_entree', '_sortie'),\n",
    "                    how='inner'\n",
    "                )\n",
    "                \n",
    "                if len(merged) > 0:\n",
    "                    # Calculer les rendements pour les param√®tres cl√©s\n",
    "                    parametres = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'Azote_Total_mg_L', 'Phosphates_mg_L']\n",
    "                    \n",
    "                    for param in parametres:\n",
    "                        col_entree = f\"{param}_entree\"\n",
    "                        col_sortie = f\"{param}_sortie\"\n",
    "                        \n",
    "                        if col_entree in merged.columns and col_sortie in merged.columns:\n",
    "                            merged[f\"Rendement_{param.replace('_mg_L', '')}_pct\"] = (\n",
    "                                (merged[col_entree] - merged[col_sortie]) / \n",
    "                                merged[col_entree] * 100\n",
    "                            ).round(2)\n",
    "                    \n",
    "                    performance_results.append(merged)\n",
    "        \n",
    "        return performance_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale\n",
    "    \"\"\"\n",
    "    # V√©rifier que le fichier existe\n",
    "    excel_file = \"Excel donn√©s qualit√© UGB.xlsx\"\n",
    "    \n",
    "    if not Path(excel_file).exists():\n",
    "        print(f\"‚ùå ERREUR : Fichier '{excel_file}' introuvable !\")\n",
    "        print(\"üìÅ Fichiers Excel trouv√©s dans le r√©pertoire actuel :\")\n",
    "        excel_files = list(Path('.').glob('*.xlsx'))\n",
    "        if excel_files:\n",
    "            for i, file in enumerate(excel_files, 1):\n",
    "                print(f\"   {i}. {file.name}\")\n",
    "            print(\"\\nüí° Conseil : V√©rifiez le nom exact du fichier ci-dessus\")\n",
    "        else:\n",
    "            print(\"   Aucun fichier .xlsx trouv√©\")\n",
    "        return\n",
    "    \n",
    "    # Initialiser le processeur\n",
    "    processor = UGBDataProcessor()\n",
    "    \n",
    "    # Traiter les donn√©es\n",
    "    result = processor.create_unified_dataset(excel_file)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"√âchec du traitement\")\n",
    "        return\n",
    "    \n",
    "    df_unified, performance_sheets, missing_analysis = result\n",
    "    \n",
    "    # Afficher un aper√ßu\n",
    "    print(\"\\n=== APER√áU DU DATASET FINAL ===\")\n",
    "    print(f\"Shape: {df_unified.shape}\")\n",
    "    print(\"\\nPremi√®res lignes :\")\n",
    "    print(df_unified.head())\n",
    "    \n",
    "    print(\"\\nColonnes disponibles :\")\n",
    "    for i, col in enumerate(df_unified.columns):\n",
    "        print(f\"{i+1:2d}. {col}\")\n",
    "    \n",
    "    # Sauvegarder le dataset principal en CSV\n",
    "    output_file_csv = \"UGB_Sanar_Station_Dataset_Clean.csv\"\n",
    "    df_unified.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Dataset principal sauvegard√© : {output_file_csv}\")\n",
    "    \n",
    "    # Sauvegarder l'analyse des valeurs manquantes en CSV\n",
    "    missing_file_csv = \"UGB_Missing_Values_Analysis.csv\"\n",
    "    missing_analysis.to_csv(missing_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Analyse des valeurs manquantes : {missing_file_csv}\")\n",
    "    \n",
    "    # Analyser les performances si demand√©\n",
    "    performance_results = processor.create_performance_analysis(df_unified, performance_sheets)\n",
    "    \n",
    "    if performance_results:\n",
    "        df_performance = pd.concat(performance_results, ignore_index=True)\n",
    "        performance_file_csv = \"UGB_Performance_Analysis.csv\"\n",
    "        df_performance.to_csv(performance_file_csv, index=False, encoding='utf-8')\n",
    "        print(f\"‚úì Analyse des performances : {performance_file_csv}\")\n",
    "    \n",
    "    print(\"\\n=== TRAITEMENT TERMIN√â ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2682f490-c5a1-4d5b-a5bc-3ddcec8d3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âBUT DU PR√âTRAITEMENT UGB ===\n",
      "Nombre de feuilles charg√©es : 13\n",
      "Traitement de la feuille : Entree 1\n",
      "  ‚úì Entree 1: 16 lignes\n",
      "Traitement de la feuille : Entree FV1\n",
      "  ‚úì Entree FV1: 16 lignes\n",
      "Traitement de la feuille : SFV1a\n",
      "  ‚úì SFV1a: 16 lignes\n",
      "Traitement de la feuille : SFV1b\n",
      "  ‚úì SFV1b: 16 lignes\n",
      "Traitement de la feuille : SFV1c\n",
      "  ‚úì SFV1c: 16 lignes\n",
      "Traitement de la feuille : Entr√©√© FV2\n",
      "  ‚úì Entr√©√© FV2: 16 lignes\n",
      "Traitement de la feuille : SFV2a\n",
      "  ‚úì SFV2a: 16 lignes\n",
      "Traitement de la feuille : SFV2b\n",
      "  ‚úì SFV2b: 16 lignes\n",
      "Traitement de la feuille : SFHa\n",
      "  ‚úì SFHa: 16 lignes\n",
      "Traitement de la feuille : SFHb\n",
      "  ‚úì SFHb: 16 lignes\n",
      "\n",
      "Dataset unifi√© cr√©√© : 160 lignes, 27 colonnes\n",
      "\n",
      "=== ANALYSE DES VALEURS MANQUANTES ===\n",
      " 10 des colonnes avec le plus de valeurs manquantes :\n",
      "                   Colonne  Valeurs_Manquantes  Pourcentage_Manquant\n",
      "                      Mois                 160                100.00\n",
      "         Heure_Echantillon                 160                100.00\n",
      "      Debit_Entree_m3_jour                 160                100.00\n",
      "Charge_Hydraulique_cm_jour                 160                100.00\n",
      "                 Operation                 160                100.00\n",
      "              Observations                 160                100.00\n",
      "          Oeufs_Helminthes                 130                 81.25\n",
      "                   MVS_pct                 112                 70.00\n",
      "           Huiles_Graisses                 112                 70.00\n",
      "             Temperature_C                 102                 63.75\n",
      "\n",
      "Colonnes num√©riques : 15\n",
      "Valeurs manquantes moyennes (param√®tres num√©riques) : 49.5%\n",
      "\n",
      "=== GESTION DES VALEURS MANQUANTES (strat√©gie: hybrid) ===\n",
      "Approche hybride intelligente...\n",
      "  ‚Üí Lignes vides supprim√©es : 38\n",
      "  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : 5\n",
      "    Colonnes : ['Heure_Echantillon', 'Debit_Entree_m3_jour', 'Operation', 'Observations', 'Charge_Hydraulique_cm_jour']\n",
      "  ‚Üí Valeurs interpol√©es (param√®tres stables) : 11\n",
      "Shape avant : (160, 27)\n",
      "Shape apr√®s : (122, 23)\n",
      "Donn√©es conserv√©es : 76.2% des lignes, 85.2% des colonnes\n",
      "\n",
      "=== POST-TRAITEMENT ===\n",
      "Donn√©es manquantes finales : 790/2806 (28.2%)\n",
      "R√©partition par phase :\n",
      "  - Entree: 38 lignes\n",
      "  - Sortie: 84 lignes\n",
      "\n",
      "=== APER√áU DU DATASET FINAL ===\n",
      "Shape: (122, 23)\n",
      "\n",
      "Premi√®res lignes :\n",
      "      ID_Station   Phase        Type_Filtre ID_Filtre       Date  Mois  \\\n",
      "0  Sanar_Station  Entree     Non_Applicable       FV1 2019-04-09   NaN   \n",
      "1  Sanar_Station  Entree     Non_Applicable       FV2 2019-04-09   NaN   \n",
      "2  Sanar_Station  Entree     Non_Applicable   General 2019-04-09   NaN   \n",
      "3  Sanar_Station  Sortie  Filtre_Horizontal        FH 2019-04-09   NaN   \n",
      "4  Sanar_Station  Sortie    Filtre_Vertical       FV1 2019-04-09   NaN   \n",
      "\n",
      "   Temperature_C    pH  Conductivite_uS_cm  Potentiel_Redox_mV  ...  MVS_pct  \\\n",
      "0           27.2  7.28              1267.0                 NaN  ...      NaN   \n",
      "1            NaN   NaN                 NaN                 NaN  ...      NaN   \n",
      "2           26.7  7.56              1270.0                 NaN  ...      NaN   \n",
      "3           24.5  8.06              1016.0                 NaN  ...      NaN   \n",
      "4           27.7  7.84              1024.0                 NaN  ...      NaN   \n",
      "\n",
      "   Nitrates_mg_L  Ammonium_mg_L  Azote_Total_mg_L  Phosphates_mg_L  \\\n",
      "0      18.070000      55.000000             110.0        69.600000   \n",
      "1      66.666667     443.333333             230.0       338.333333   \n",
      "2      20.330000      20.800000             100.0        43.000000   \n",
      "3       6.320000       3.200000              40.0         2.000000   \n",
      "4      11.300000      21.000000              90.0        46.000000   \n",
      "\n",
      "   Coliformes_Fecaux_CFU_100ml  Oeufs_Helminthes  Huiles_Graisses  \\\n",
      "0                 2.670000e+06               NaN              NaN   \n",
      "1                 2.733333e+06               NaN              NaN   \n",
      "2                 3.000000e+07               NaN              NaN   \n",
      "3                 0.000000e+00               NaN              NaN   \n",
      "4                 3.670000e+06               NaN              NaN   \n",
      "\n",
      "   Nom_Feuille  Contient_Valeurs_Estimees  \n",
      "0   Entree FV1                      False  \n",
      "1   Entr√©√© FV2                       True  \n",
      "2     Entree 1                      False  \n",
      "3         SFHa                      False  \n",
      "4        SFV1a                      False  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Colonnes disponibles :\n",
      " 1. ID_Station\n",
      " 2. Phase\n",
      " 3. Type_Filtre\n",
      " 4. ID_Filtre\n",
      " 5. Date\n",
      " 6. Mois\n",
      " 7. Temperature_C\n",
      " 8. pH\n",
      " 9. Conductivite_uS_cm\n",
      "10. Potentiel_Redox_mV\n",
      "11. DBO5_mg_L\n",
      "12. DCO_mg_L\n",
      "13. MES_mg_L\n",
      "14. MVS_pct\n",
      "15. Nitrates_mg_L\n",
      "16. Ammonium_mg_L\n",
      "17. Azote_Total_mg_L\n",
      "18. Phosphates_mg_L\n",
      "19. Coliformes_Fecaux_CFU_100ml\n",
      "20. Oeufs_Helminthes\n",
      "21. Huiles_Graisses\n",
      "22. Nom_Feuille\n",
      "23. Contient_Valeurs_Estimees\n",
      "‚úì Dataset principal sauvegard√© : UGB_Sanar_Station_Dataset_Clean.csv\n",
      "‚úì Analyse des valeurs manquantes : UGB_Missing_Values_Analysis.csv\n",
      "\n",
      "=== DIAGNOSTIC AVANT PERFORMANCE ===\n",
      "Colonnes dans df_unified : ['ID_Station', 'Phase', 'Type_Filtre', 'ID_Filtre', 'Date', 'Mois', 'Temperature_C', 'pH', 'Conductivite_uS_cm', 'Potentiel_Redox_mV', 'DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'MVS_pct', 'Nitrates_mg_L', 'Ammonium_mg_L', 'Azote_Total_mg_L', 'Phosphates_mg_L', 'Coliformes_Fecaux_CFU_100ml', 'Oeufs_Helminthes', 'Huiles_Graisses', 'Nom_Feuille', 'Contient_Valeurs_Estimees']\n",
      "Dates valides : 62/122\n",
      "Phases disponibles : ['Entree' 'Sortie']\n",
      "Filtres disponibles : ['FV1' 'FV2' 'General' 'FH']\n",
      "Donn√©es d'entr√©e : 38 lignes\n",
      "Donn√©es de sortie : 84 lignes\n",
      "  - FV1: 28 entr√©es, 36 sorties\n",
      "  - FV2: 24 entr√©es, 25 sorties\n",
      "\n",
      "=== ANALYSE DES PERFORMANCES ===\n",
      "Calcul des rendements pour FV1\n",
      "Calcul des rendements pour FV2\n",
      "‚úì Analyse des performances : UGB_Performance_Analysis.csv\n",
      "\n",
      "=== TRAITEMENT TERMIN√â ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UGBDataProcessor:\n",
    "    \"\"\"\n",
    "    Classe pour le pr√©traitement des donn√©es de qualit√© d'eau UGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mapping pour uniformiser les noms de colonnes\n",
    "        self.column_mapping = {\n",
    "            # Colonnes temporelles\n",
    "            'Mois': 'Mois',\n",
    "            'Data': 'Date',\n",
    "            'Heure echantillon ': 'Heure_Echantillon',\n",
    "            'Heure echantillon /observations': 'Heure_Echantillon',\n",
    "            \n",
    "            # Param√®tres hydrauliques\n",
    "            'Debit entree (m3/jour)': 'Debit_Entree_m3_jour',\n",
    "            'Charge hydraulique (cm/jour)': 'Charge_Hydraulique_cm_jour',\n",
    "            \n",
    "            # Param√®tres physico-chimiques\n",
    "            'Temperature (¬∫C)': 'Temperature_C',\n",
    "            'pH': 'pH',\n",
    "            'CE (¬µSm/cm)': 'Conductivite_uS_cm',\n",
    "            'CE (¬µsm/cm)': 'Conductivite_uS_cm',\n",
    "            'Redox': 'Potentiel_Redox_mV',\n",
    "            \n",
    "            # Param√®tres de pollution\n",
    "            'DBO5 (mg/L)': 'DBO5_mg_L',\n",
    "            'DCO (mg/L)': 'DCO_mg_L',\n",
    "            'MeS (mg/L)': 'MES_mg_L',\n",
    "            'MVS (%)': 'MVS_pct',\n",
    "            \n",
    "            # Param√®tres azot√©s (TOUTES LES VARIANTES vers le m√™me nom)\n",
    "            'Nitrates (mgNO3-/l)': 'Nitrates_mg_L',\n",
    "            'Ammonium (mgNH4-/l) ': 'Ammonium_mg_L',\n",
    "            'Azot total (mgN/l)': 'Azote_Total_mg_L',      # Variante 1\n",
    "            'Azote total (mgN/l)': 'Azote_Total_mg_L',     # Variante 2 \n",
    "            'Azote toal (mgN/l)': 'Azote_Total_mg_L',      # Variante 3 (typo)\n",
    "            \n",
    "            # Param√®tres phosphor√©s\n",
    "            'Phosphates (mgPO4-/l)': 'Phosphates_mg_L',\n",
    "            \n",
    "            # Param√®tres microbiologiques\n",
    "            'Coliformes f√©caux (CFU/100ml)': 'Coliformes_Fecaux_CFU_100ml',\n",
    "            'Oeufs helmint': 'Oeufs_Helminthes',\n",
    "            \n",
    "            # Autres\n",
    "            'Huiles et graisses': 'Huiles_Graisses',\n",
    "            'Op√©ration': 'Operation',\n",
    "            'Observation': 'Observations'\n",
    "        }\n",
    "        \n",
    "        # D√©finition des types de feuilles et leurs caract√©ristiques\n",
    "        self.sheet_config = {\n",
    "            'entree': {\n",
    "                'sheets': ['Entree 1', 'Entree FV1', 'Entr√©√© FV2'],\n",
    "                'phase': 'Entree',\n",
    "                'description': 'Eaux d\\'entr√©e'\n",
    "            },\n",
    "            'sortie_fv': {\n",
    "                'sheets': ['SFV1a', 'SFV1b', 'SFV1c', 'SFV2a', 'SFV2b'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Vertical',\n",
    "                'description': 'Sorties filtres verticaux'\n",
    "            },\n",
    "            'sortie_fh': {\n",
    "                'sheets': ['SFHa', 'SFHb'],\n",
    "                'phase': 'Sortie',\n",
    "                'type_filtre': 'Filtre_Horizontal',\n",
    "                'description': 'Sorties filtres horizontaux'\n",
    "            },\n",
    "            'performance': {\n",
    "                'sheets': ['% √©l FV1', '% √©l FV2', '% √©l de la station'],\n",
    "                'phase': 'Performance',\n",
    "                'description': 'Donn√©es de performance'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def clean_value(self, x):\n",
    "        \"\"\"\n",
    "        Nettoie une valeur individuelle\n",
    "        G√®re sp√©cifiquement : <2000, >500, \"pas de donn√©e\", etc.\n",
    "        \"\"\"\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        \n",
    "        if isinstance(x, str):\n",
    "            x = x.strip()\n",
    "            \n",
    "            # G√©rer les valeurs sp√©ciales (valeurs manquantes)\n",
    "            missing_indicators = ['pas de donn√©e', 'nd', 'n.d.', '-', 'absent', '', 'nan', 'null']\n",
    "            if any(term in x.lower() for term in missing_indicators):\n",
    "                return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec > (ex: \">500\" devient 500)\n",
    "            # Interpr√©tation: valeur minimale possible\n",
    "            if '>' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('>', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer les valeurs avec < (ex: \"<2000\" devient 2000) \n",
    "            # Interpr√©tation: valeur maximale possible (limite de d√©tection)\n",
    "            if '<' in x:\n",
    "                try:\n",
    "                    numeric_part = x.replace('<', '').strip()\n",
    "                    return float(numeric_part)\n",
    "                except ValueError:\n",
    "                    return np.nan\n",
    "            \n",
    "            # G√©rer d'autres formats possibles\n",
    "            # Retirer les espaces et caract√®res parasites\n",
    "            x = x.replace(',', '.').replace(' ', '')\n",
    "            \n",
    "            # Essayer de convertir en num√©rique\n",
    "            try:\n",
    "                return float(x)\n",
    "            except ValueError:\n",
    "                # Si ce n'est pas num√©rique, retourner tel quel (texte)\n",
    "                return x if x else np.nan\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def standardize_columns(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Standardise les noms de colonnes d'un DataFrame\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Renommer les colonnes selon le mapping\n",
    "        df_clean.rename(columns=self.column_mapping, inplace=True)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es\n",
    "        df_clean['ID_Station'] = 'Sanar_Station'\n",
    "        df_clean['Nom_Feuille'] = sheet_name\n",
    "        \n",
    "        # D√©terminer le type et la phase selon la feuille\n",
    "        for config_type, config in self.sheet_config.items():\n",
    "            if sheet_name in config['sheets']:\n",
    "                df_clean['Phase'] = config['phase']\n",
    "                if 'type_filtre' in config:\n",
    "                    df_clean['Type_Filtre'] = config['type_filtre']\n",
    "                else:\n",
    "                    df_clean['Type_Filtre'] = 'Non_Applicable'\n",
    "                break\n",
    "        \n",
    "        # Identifier le filtre sp√©cifique\n",
    "        if 'FV1' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV1'\n",
    "        elif 'FV2' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FV2'\n",
    "        elif 'FH' in sheet_name:\n",
    "            df_clean['ID_Filtre'] = 'FH'\n",
    "        else:\n",
    "            df_clean['ID_Filtre'] = 'General'\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def process_sheet(self, df, sheet_name):\n",
    "        \"\"\"\n",
    "        Traite une feuille individuelle\n",
    "        \"\"\"\n",
    "        print(f\"Traitement de la feuille : {sheet_name}\")\n",
    "        \n",
    "        # Standardiser les colonnes\n",
    "        df_processed = self.standardize_columns(df, sheet_name)\n",
    "        \n",
    "        # Nettoyer les valeurs (sauf les m√©tadonn√©es)\n",
    "        metadata_cols = ['ID_Station', 'Nom_Feuille', 'Phase', 'Type_Filtre', 'ID_Filtre']\n",
    "        \n",
    "        for col in df_processed.columns:\n",
    "            if col not in metadata_cols:\n",
    "                df_processed[col] = df_processed[col].apply(self.clean_value)\n",
    "        \n",
    "        # Traiter la date\n",
    "        if 'Date' in df_processed.columns:\n",
    "            df_processed['Date'] = pd.to_datetime(df_processed['Date'], errors='coerce')\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def analyze_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Analyse d√©taill√©e des valeurs manquantes\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES VALEURS MANQUANTES ===\")\n",
    "        \n",
    "        # Compter les valeurs manquantes par colonne\n",
    "        missing_stats = []\n",
    "        for col in df.columns:\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            total_count = len(df)\n",
    "            missing_pct = (missing_count / total_count) * 100\n",
    "            \n",
    "            # Analyser les types de valeurs non-manquantes\n",
    "            non_missing = df[col].dropna()\n",
    "            if len(non_missing) > 0:\n",
    "                if col in ['Date', 'Mois', 'Heure_Echantillon', 'ID_Station', 'Phase', 'Type_Filtre']:\n",
    "                    data_type = 'M√©tadonn√©e'\n",
    "                elif non_missing.dtype in ['float64', 'int64']:\n",
    "                    data_type = 'Num√©rique'\n",
    "                else:\n",
    "                    data_type = 'Texte'\n",
    "            else:\n",
    "                data_type = 'Vide'\n",
    "            \n",
    "            missing_stats.append({\n",
    "                'Colonne': col,\n",
    "                'Valeurs_Manquantes': missing_count,\n",
    "                'Total': total_count,\n",
    "                'Pourcentage_Manquant': missing_pct,\n",
    "                'Type_Donnee': data_type\n",
    "            })\n",
    "        \n",
    "        missing_df = pd.DataFrame(missing_stats)\n",
    "        missing_df = missing_df.sort_values('Pourcentage_Manquant', ascending=False)\n",
    "        \n",
    "        # Afficher le top 10 des colonnes avec le plus de valeurs manquantes\n",
    "        print(\" 10 des colonnes avec le plus de valeurs manquantes :\")\n",
    "        print(missing_df.head(10)[['Colonne', 'Valeurs_Manquantes', 'Pourcentage_Manquant']].to_string(index=False))\n",
    "        \n",
    "        # Statistiques globales\n",
    "        colonnes_numeriques = missing_df[missing_df['Type_Donnee'] == 'Num√©rique']\n",
    "        if len(colonnes_numeriques) > 0:\n",
    "            print(f\"\\nColonnes num√©riques : {len(colonnes_numeriques)}\")\n",
    "            print(f\"Valeurs manquantes moyennes (param√®tres num√©riques) : {colonnes_numeriques['Pourcentage_Manquant'].mean():.1f}%\")\n",
    "        \n",
    "        return missing_df\n",
    "    \n",
    "    def handle_missing_values(self, df, strategy='hybrid'):\n",
    "        \"\"\"\n",
    "        G√®re les valeurs manquantes selon diff√©rentes strat√©gies\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== GESTION DES VALEURS MANQUANTES (strat√©gie: {strategy}) ===\")\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        initial_shape = df_clean.shape\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # Supprimer les lignes compl√®tement vides (toutes les valeurs num√©riques sont NaN)\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            print(f\"Lignes compl√®tement vides supprim√©es : {initial_shape[0] - len(df_clean)}\")\n",
    "            \n",
    "        elif strategy == 'aggressive':\n",
    "            # Supprimer les colonnes avec plus de 80% de valeurs manquantes\n",
    "            threshold = 0.8\n",
    "            cols_to_drop = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date']:  # Garder les m√©tadonn√©es importantes\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > threshold:\n",
    "                        cols_to_drop.append(col)\n",
    "            \n",
    "            df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "            print(f\"Colonnes supprim√©es (>{threshold*100}% manquantes) : {len(cols_to_drop)}\")\n",
    "            if cols_to_drop:\n",
    "                print(f\"Colonnes supprim√©es : {cols_to_drop}\")\n",
    "                \n",
    "        elif strategy == 'hybrid':\n",
    "            # APPROCHE HYBRIDE - La meilleure pour les donn√©es de qualit√© d'eau\n",
    "            print(\"Approche hybride intelligente...\")\n",
    "            \n",
    "            # √âtape 1: Supprimer les lignes compl√®tement vides\n",
    "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "            rows_before = len(df_clean)\n",
    "            df_clean = df_clean.dropna(subset=numeric_cols, how='all')\n",
    "            rows_removed = rows_before - len(df_clean)\n",
    "            print(f\"  ‚Üí Lignes vides supprim√©es : {rows_removed}\")\n",
    "            \n",
    "            # √âtape 2: Identifier les colonnes tr√®s probl√©matiques (>95% manquantes)\n",
    "            very_empty_cols = []\n",
    "            for col in df_clean.columns:\n",
    "                if col not in ['ID_Station', 'Phase', 'Type_Filtre', 'Date', 'Mois']:\n",
    "                    missing_pct = df_clean[col].isnull().sum() / len(df_clean)\n",
    "                    if missing_pct > 0.95:  # Plus strict que aggressive\n",
    "                        very_empty_cols.append(col)\n",
    "            \n",
    "            if very_empty_cols:\n",
    "                df_clean = df_clean.drop(columns=very_empty_cols)\n",
    "                print(f\"  ‚Üí Colonnes quasi-vides supprim√©es (>95% manquantes) : {len(very_empty_cols)}\")\n",
    "                print(f\"    Colonnes : {very_empty_cols}\")\n",
    "            \n",
    "            # √âtape 3: Interpolation douce pour les param√®tres physiques stables\n",
    "            stable_params = ['Temperature_C', 'pH', 'Conductivite_uS_cm']\n",
    "            interpolated_count = 0\n",
    "            \n",
    "            for param in stable_params:\n",
    "                if param in df_clean.columns and 'Date' in df_clean.columns:\n",
    "                    # Interpolation seulement si max 1 valeur manquante cons√©cutive\n",
    "                    df_clean = df_clean.sort_values(['Date', 'Phase', 'ID_Filtre'])\n",
    "                    original_nulls = df_clean[param].isnull().sum()\n",
    "                    \n",
    "                    df_clean[param] = df_clean.groupby(['Phase', 'ID_Filtre'])[param].transform(\n",
    "                        lambda x: x.interpolate(method='linear', limit=1)  # Max 1 trou\n",
    "                    )\n",
    "                    \n",
    "                    final_nulls = df_clean[param].isnull().sum()\n",
    "                    if original_nulls > final_nulls:\n",
    "                        interpolated_count += (original_nulls - final_nulls)\n",
    "            \n",
    "            if interpolated_count > 0:\n",
    "                print(f\"  ‚Üí Valeurs interpol√©es (param√®tres stables) : {interpolated_count}\")\n",
    "            \n",
    "            # √âtape 4: Marquer les valeurs estim√©es vs mesur√©es\n",
    "            df_clean['Contient_Valeurs_Estimees'] = False\n",
    "            main_params = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'pH', 'Temperature_C']\n",
    "            available_main = [p for p in main_params if p in df_clean.columns]\n",
    "            \n",
    "            if available_main:\n",
    "                missing_main = df_clean[available_main].isnull().sum(axis=1)\n",
    "                df_clean.loc[missing_main >= len(available_main) * 0.3, 'Contient_Valeurs_Estimees'] = True\n",
    "        \n",
    "        final_shape = df_clean.shape\n",
    "        print(f\"Shape avant : {initial_shape}\")\n",
    "        print(f\"Shape apr√®s : {final_shape}\")\n",
    "        print(f\"Donn√©es conserv√©es : {(final_shape[0]/initial_shape[0]*100):.1f}% des lignes, \"\n",
    "              f\"{(final_shape[1]/initial_shape[1]*100):.1f}% des colonnes\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "    def post_process(self, df):\n",
    "        \"\"\"\n",
    "        Post-traitement du dataset unifi√©\n",
    "        \"\"\"\n",
    "        print(\"\\n=== POST-TRAITEMENT ===\")\n",
    "        \n",
    "        # R√©organiser les colonnes par ordre de priorit√©\n",
    "        priority_cols = ['ID_Station', 'Phase', 'Type_Filtre', 'ID_Filtre', 'Date', 'Mois', 'Heure_Echantillon']\n",
    "        available_priority = [col for col in priority_cols if col in df.columns]\n",
    "        other_cols = [col for col in df.columns if col not in priority_cols]\n",
    "        df = df[available_priority + other_cols]\n",
    "        \n",
    "        # Trier par date si possible\n",
    "        if 'Date' in df.columns:\n",
    "            df = df.sort_values(['Date', 'Phase', 'ID_Filtre']).reset_index(drop=True)\n",
    "        \n",
    "        # Statistiques de nettoyage finales\n",
    "        total_cells = df.shape[0] * df.shape[1]\n",
    "        missing_cells = df.isnull().sum().sum()\n",
    "        missing_pct = (missing_cells / total_cells) * 100\n",
    "        \n",
    "        print(f\"Donn√©es manquantes finales : {missing_cells}/{total_cells} ({missing_pct:.1f}%)\")\n",
    "        \n",
    "        # R√©sum√© par phase\n",
    "        if 'Phase' in df.columns:\n",
    "            phase_summary = df.groupby('Phase').size()\n",
    "            print(\"R√©partition par phase :\")\n",
    "            for phase, count in phase_summary.items():\n",
    "                print(f\"  - {phase}: {count} lignes\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_unified_dataset(self, excel_file_path):\n",
    "        \"\"\"\n",
    "        Cr√©e un dataset unifi√© √† partir du fichier Excel\n",
    "        \"\"\"\n",
    "        print(\"=== D√âBUT DU PR√âTRAITEMENT UGB ===\")\n",
    "        \n",
    "        # 1. Charger toutes les feuilles\n",
    "        try:\n",
    "            df_dict = pd.read_excel(excel_file_path, sheet_name=None, header=3)\n",
    "\n",
    "            print(f\"Nombre de feuilles charg√©es : {len(df_dict)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement : {e}\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Traiter chaque feuille (exclure les feuilles de performance pour l'instant)\n",
    "        processed_sheets = []\n",
    "        performance_sheets = []\n",
    "        \n",
    "        for sheet_name, data in df_dict.items():\n",
    "            if sheet_name.startswith('% √©l'):\n",
    "                # Traiter s√©par√©ment les feuilles de performance\n",
    "                performance_sheets.append((sheet_name, data))\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                processed_df = self.process_sheet(data, sheet_name)\n",
    "                processed_sheets.append(processed_df)\n",
    "                print(f\"  ‚úì {sheet_name}: {len(processed_df)} lignes\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Erreur avec {sheet_name}: {e}\")\n",
    "        \n",
    "        # 3. Combiner toutes les feuilles\n",
    "        if processed_sheets:\n",
    "            df_unified = pd.concat(processed_sheets, ignore_index=True, sort=False)\n",
    "            print(f\"\\nDataset unifi√© cr√©√© : {len(df_unified)} lignes, {len(df_unified.columns)} colonnes\")\n",
    "        else:\n",
    "            print(\"Aucune feuille trait√©e avec succ√®s\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Analyser les valeurs manquantes\n",
    "        missing_analysis = self.analyze_missing_values(df_unified)\n",
    "        \n",
    "        # 5. G√©rer les valeurs manquantes avec strat√©gie hybrid\n",
    "        df_unified = self.handle_missing_values(df_unified, strategy='hybrid')\n",
    "        \n",
    "        # 6. Post-traitement final\n",
    "        df_unified = self.post_process(df_unified)\n",
    "        \n",
    "        return df_unified, performance_sheets, missing_analysis\n",
    "\n",
    "    def create_performance_analysis(self, df_unified, performance_sheets):\n",
    "        \"\"\"\n",
    "        Analyse les donn√©es de performance si disponibles\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ANALYSE DES PERFORMANCES ===\")\n",
    "        \n",
    "        # Cr√©er des paires entr√©e-sortie pour calcul des rendements\n",
    "        paires_filtres = {\n",
    "            'FV1': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV1', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV1')]\n",
    "            },\n",
    "            'FV2': {\n",
    "                'entree': df_unified[(df_unified['Phase'] == 'Entree') & \n",
    "                                   (df_unified['ID_Filtre'].isin(['FV2', 'General']))],\n",
    "                'sortie': df_unified[(df_unified['Phase'] == 'Sortie') & \n",
    "                                   (df_unified['ID_Filtre'] == 'FV2')]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        performance_results = []\n",
    "        \n",
    "        for filtre_id, data in paires_filtres.items():\n",
    "            if len(data['entree']) > 0 and len(data['sortie']) > 0:\n",
    "                print(f\"Calcul des rendements pour {filtre_id}\")\n",
    "                \n",
    "                # Fusionner entr√©e et sortie sur la date\n",
    "                merged = pd.merge(\n",
    "                    data['entree'], \n",
    "                    data['sortie'], \n",
    "                    on='Date', \n",
    "                    suffixes=('_entree', '_sortie'),\n",
    "                    how='inner'\n",
    "                )\n",
    "                \n",
    "                if len(merged) > 0:\n",
    "                    # Calculer les rendements pour les param√®tres cl√©s\n",
    "                    parametres = ['DBO5_mg_L', 'DCO_mg_L', 'MES_mg_L', 'Azote_Total_mg_L', 'Phosphates_mg_L']\n",
    "                    \n",
    "                    for param in parametres:\n",
    "                        col_entree = f\"{param}_entree\"\n",
    "                        col_sortie = f\"{param}_sortie\"\n",
    "                        \n",
    "                        if col_entree in merged.columns and col_sortie in merged.columns:\n",
    "                            merged[f\"Rendement_{param.replace('_mg_L', '')}_pct\"] = (\n",
    "                                (merged[col_entree] - merged[col_sortie]) / \n",
    "                                merged[col_entree] * 100\n",
    "                            ).round(2)\n",
    "                    \n",
    "                    performance_results.append(merged)\n",
    "        \n",
    "        return performance_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale\n",
    "    \"\"\"\n",
    "    # V√©rifier que le fichier existe\n",
    "    excel_file = \"Excel donn√©s qualit√© UGB.xlsx\"\n",
    "    \n",
    "    if not Path(excel_file).exists():\n",
    "        print(f\"‚ùå ERREUR : Fichier '{excel_file}' introuvable !\")\n",
    "        print(\"üìÅ Fichiers Excel trouv√©s dans le r√©pertoire actuel :\")\n",
    "        excel_files = list(Path('.').glob('*.xlsx'))\n",
    "        if excel_files:\n",
    "            for i, file in enumerate(excel_files, 1):\n",
    "                print(f\"   {i}. {file.name}\")\n",
    "            print(\"\\nüí° Conseil : V√©rifiez le nom exact du fichier ci-dessus\")\n",
    "        else:\n",
    "            print(\"   Aucun fichier .xlsx trouv√©\")\n",
    "        return\n",
    "    \n",
    "    # Initialiser le processeur\n",
    "    processor = UGBDataProcessor()\n",
    "    \n",
    "    # Traiter les donn√©es\n",
    "    result = processor.create_unified_dataset(excel_file)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"√âchec du traitement\")\n",
    "        return\n",
    "    \n",
    "    # CORRECTION: R√©cup√©rer les r√©sultats AVANT le diagnostic\n",
    "    df_unified, performance_sheets, missing_analysis = result\n",
    "    \n",
    "    # Afficher un aper√ßu\n",
    "    print(\"\\n=== APER√áU DU DATASET FINAL ===\")\n",
    "    print(f\"Shape: {df_unified.shape}\")\n",
    "    print(\"\\nPremi√®res lignes :\")\n",
    "    print(df_unified.head())\n",
    "    \n",
    "    print(\"\\nColonnes disponibles :\")\n",
    "    for i, col in enumerate(df_unified.columns):\n",
    "        print(f\"{i+1:2d}. {col}\")\n",
    "    \n",
    "    # Sauvegarder le dataset principal en CSV\n",
    "    output_file_csv = \"UGB_Sanar_Station_Dataset_Clean.csv\"\n",
    "    df_unified.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Dataset principal sauvegard√© : {output_file_csv}\")\n",
    "    \n",
    "    # Sauvegarder l'analyse des valeurs manquantes en CSV\n",
    "    missing_file_csv = \"UGB_Missing_Values_Analysis.csv\"\n",
    "    missing_analysis.to_csv(missing_file_csv, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Analyse des valeurs manquantes : {missing_file_csv}\")\n",
    "    \n",
    "    # DIAGNOSTIC AVANT L'ANALYSE DE PERFORMANCE\n",
    "    print(\"\\n=== DIAGNOSTIC AVANT PERFORMANCE ===\")\n",
    "    print(f\"Colonnes dans df_unified : {list(df_unified.columns)}\")\n",
    "    if 'Date' in df_unified.columns:\n",
    "        print(f\"Dates valides : {df_unified['Date'].notna().sum()}/{len(df_unified)}\")\n",
    "        \n",
    "        # V√©rifier les phases et filtres\n",
    "        if 'Phase' in df_unified.columns:\n",
    "            print(f\"Phases disponibles : {df_unified['Phase'].unique()}\")\n",
    "        if 'ID_Filtre' in df_unified.columns:\n",
    "            print(f\"Filtres disponibles : {df_unified['ID_Filtre'].unique()}\")\n",
    "            \n",
    "        # V√©rifier les donn√©es par phase/filtre\n",
    "        if 'Phase' in df_unified.columns and 'ID_Filtre' in df_unified.columns:\n",
    "            entree_data = df_unified[df_unified['Phase'] == 'Entree']\n",
    "            sortie_data = df_unified[df_unified['Phase'] == 'Sortie']\n",
    "            print(f\"Donn√©es d'entr√©e : {len(entree_data)} lignes\")\n",
    "            print(f\"Donn√©es de sortie : {len(sortie_data)} lignes\")\n",
    "            \n",
    "            # D√©tail par filtre\n",
    "            for filtre_id in ['FV1', 'FV2']:\n",
    "                entree_filtre = entree_data[entree_data['ID_Filtre'].isin([filtre_id, 'General'])]\n",
    "                sortie_filtre = sortie_data[sortie_data['ID_Filtre'] == filtre_id]\n",
    "                print(f\"  - {filtre_id}: {len(entree_filtre)} entr√©es, {len(sortie_filtre)} sorties\")\n",
    "    else:\n",
    "        print(\"‚ùå Colonne 'Date' manquante !\")\n",
    "        print(\"Le probl√®me est dans le mapping des colonnes ou le traitement des feuilles.\")\n",
    "        return\n",
    "    \n",
    "    # Analyser les performances si demand√©\n",
    "    try:\n",
    "        performance_results = processor.create_performance_analysis(df_unified, performance_sheets)\n",
    "        \n",
    "        if performance_results:\n",
    "            df_performance = pd.concat(performance_results, ignore_index=True)\n",
    "            performance_file_csv = \"UGB_Performance_Analysis.csv\"\n",
    "            df_performance.to_csv(performance_file_csv, index=False, encoding='utf-8')\n",
    "            print(f\"‚úì Analyse des performances : {performance_file_csv}\")\n",
    "        else:\n",
    "            print(\"‚ö† Aucune analyse de performance g√©n√©r√©e\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'analyse de performance : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n=== TRAITEMENT TERMIN√â ===\")\n",
    "\n",
    "\n",
    "# Version alternative avec gestion d'erreur plus robuste\n",
    "def main_with_error_handling():\n",
    "    \"\"\"\n",
    "    Version alternative de main() avec gestion d'erreur robuste\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # V√©rifier que le fichier existe\n",
    "        excel_file = \"Excel donn√©s qualit√© UGB.xlsx\"\n",
    "        \n",
    "        if not Path(excel_file).exists():\n",
    "            print(f\"‚ùå ERREUR : Fichier '{excel_file}' introuvable !\")\n",
    "            return\n",
    "        \n",
    "        # Initialiser le processeur\n",
    "        processor = UGBDataProcessor()\n",
    "        \n",
    "        # Traiter les donn√©es\n",
    "        print(\"=== D√âBUT DU TRAITEMENT ===\")\n",
    "        result = processor.create_unified_dataset(excel_file)\n",
    "        \n",
    "        if result is None:\n",
    "            print(\"‚ùå √âchec du traitement des donn√©es\")\n",
    "            return\n",
    "        \n",
    "        df_unified, performance_sheets, missing_analysis = result\n",
    "        \n",
    "        # V√©rifications de s√©curit√©\n",
    "        if df_unified is None or len(df_unified) == 0:\n",
    "            print(\"‚ùå Dataset vide apr√®s traitement\")\n",
    "            return\n",
    "        \n",
    "        print(f\"‚úÖ Dataset trait√© avec succ√®s : {df_unified.shape}\")\n",
    "        \n",
    "        # Diagnostic d√©taill√©\n",
    "        print(\"\\n=== DIAGNOSTIC D√âTAILL√â ===\")\n",
    "        print(\"Colonnes critiques :\")\n",
    "        critical_cols = ['Date', 'Phase', 'ID_Filtre', 'Type_Filtre']\n",
    "        for col in critical_cols:\n",
    "            if col in df_unified.columns:\n",
    "                non_null = df_unified[col].notna().sum()\n",
    "                print(f\"  ‚úÖ {col}: {non_null}/{len(df_unified)} valeurs\")\n",
    "                if col in ['Phase', 'ID_Filtre']:\n",
    "                    print(f\"      Valeurs uniques: {list(df_unified[col].dropna().unique())}\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {col}: MANQUANTE\")\n",
    "        \n",
    "        # Sauvegardes\n",
    "        try:\n",
    "            output_file_csv = \"UGB_Sanar_Station_Dataset_Clean.csv\"\n",
    "            df_unified.to_csv(output_file_csv, index=False, encoding='utf-8')\n",
    "            print(f\"\\n‚úÖ Dataset sauvegard√© : {output_file_csv}\")\n",
    "            \n",
    "            missing_file_csv = \"UGB_Missing_Values_Analysis.csv\"\n",
    "            missing_analysis.to_csv(missing_file_csv, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Analyse missing values : {missing_file_csv}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de la sauvegarde : {e}\")\n",
    "        \n",
    "        # Analyse de performance (optionnelle)\n",
    "        if 'Date' in df_unified.columns and 'Phase' in df_unified.columns:\n",
    "            print(\"\\n=== TENTATIVE D'ANALYSE DE PERFORMANCE ===\")\n",
    "            try:\n",
    "                performance_results = processor.create_performance_analysis(df_unified, performance_sheets)\n",
    "                \n",
    "                if performance_results and len(performance_results) > 0:\n",
    "                    df_performance = pd.concat(performance_results, ignore_index=True)\n",
    "                    performance_file_csv = \"UGB_Performance_Analysis.csv\"\n",
    "                    df_performance.to_csv(performance_file_csv, index=False, encoding='utf-8')\n",
    "                    print(f\"‚úÖ Analyse de performance : {performance_file_csv}\")\n",
    "                else:\n",
    "                    print(\"‚ö† Aucune donn√©e de performance g√©n√©r√©e\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† √âchec de l'analyse de performance (non critique) : {e}\")\n",
    "        else:\n",
    "            print(\"‚ö† Analyse de performance ignor√©e (colonnes manquantes)\")\n",
    "        \n",
    "        print(\"\\nüéâ TRAITEMENT TERMIN√â AVEC SUCC√àS\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR CRITIQUE : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choisissez la version que vous pr√©f√©rez\n",
    "    main()  # Version avec diagnostic\n",
    "    # main_with_error_handling()  # Version ultra-robuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b7ac606-6364-4a40-b0c7-c26e1d761ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supposons que votre DataFrame nettoy√© est nomm√© df_unified\n",
    "# Si ce n'est pas d√©j√† fait, chargez le fichier CSV dans un DataFrame\n",
    "df_unified = pd.read_csv(\"UGB_Sanar_Station_Dataset_Clean.csv\")\n",
    "\n",
    "# Affichez le nombre de lignes et de colonnes\n",
    "print(df_unified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4a863-cad0-4678-bab6-ea11e83e2fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
